{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a08e931",
   "metadata": {},
   "source": [
    "# 4 テキストを生成するための GPT モデルを一から実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622ffa3",
   "metadata": {},
   "source": [
    "## 4.1. LLM アーキテクチャのコーディング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44971ea3",
   "metadata": {},
   "source": [
    "**GPT** : Generative Pretrained Transformer  \n",
    "パラメータ -> モデルの訓練可能な重み。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8908b41",
   "metadata": {},
   "source": [
    "GPT-2 では 12,400,000 パラメータ (公開済み) ->  こちらの実装  \n",
    "GPT-3 では 175,000,000,000 パラメータ (本の執筆時では未公開)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1ef126",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # トークンの数 (語彙の個数)\n",
    "    \"context_length\": 1024,  # コンテキストの長さ (入力トークンの最大数)\n",
    "    \"emb_dim\": 768,  # 埋め込みの次元数 \n",
    "    \"n_heads\": 12,  # Attention のヘッドの数\n",
    "    \"n_layers\": 12,  # Transformer のブロック数\n",
    "    \"drop_rate\": 0.1,  # ドロップアウト率 -> 0.1 は 隠れ層ユニットの 10% がランダムに無効化\n",
    "    \"qkv_bias\": False # 現代の研究では基本無効だが、GPT-2 の重みを OpenAI から読み込むときに True にする\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fec3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "            \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "            )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x  # ダミーの実装なので何もしない\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x  # ダミーの実装なので何もしない"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1310a",
   "metadata": {},
   "source": [
    "**ロジット** (Rogits) : モデルの出力のこと"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfda151",
   "metadata": {},
   "source": [
    "`DummyTransformerBlock` と `DummyLayerNorm` は後で実装する。まずは処理の流れを forward に従って実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a9a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8b106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6109, 3626, 6100, 345]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(txt1))\n",
    "print(type(tokenizer.encode(txt1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10c99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d729496d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)  # 再現性のためのシード設定\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce35cec",
   "metadata": {},
   "source": [
    "## 4.2 層正規化を使って活性化を正規化する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339ed93",
   "metadata": {},
   "source": [
    "**層正規化** (layer normalization)：ニューラルネットワーク層の平均 0 分散 1 になるように調整すること。  \n",
    "-> 勾配の消失や爆発を防ぎ、効果的な重みへの収束を早める。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17c363",
   "metadata": {},
   "source": [
    "**ReLU** (Rectified Linear Unit) : 負の値を 0 にするだけ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1304a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]]) \n",
      "\n",
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)  # 再現性のためのシード設定\n",
    "batch_example = torch.randn(2, 5) # ダミーのバッチデータ　5つの次元 (特徴量) をもつ、2 つの訓練サンプル\n",
    "print(batch_example, \"\\n\")\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU()) # 5 -> 6次元線形変換する層を経た後、ReLU 活性化関数を適用する層の実装\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0ac10",
   "metadata": {},
   "source": [
    "各行平均と分散を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26a4d9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=1, keepdim=True)\n",
    "var = out.var(dim=1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e69a57",
   "metadata": {},
   "source": [
    "`keepdim=True`: `dim` で指定した次元に沿って平均を計算する。指定しない場合は次のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f217bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([0.1324, 0.2170], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([0.0231, 0.0398], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean_ = out.mean(dim=1)\n",
    "var_ = out.var(dim=1)\n",
    "print(\"Mean:\\n\", mean_)\n",
    "print(\"Variance:\\n\", var_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92d7a5",
   "metadata": {},
   "source": [
    "**標準化** (normarizetion) : 平均で引いて分散で割る  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec98369",
   "metadata": {},
   "source": [
    "層の出力に層正規化を適用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1efed20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7461090",
   "metadata": {},
   "source": [
    "科学表記 (`e-09`) とかをオフにできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6908c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0900b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # 不変ではなく有偏分散。n がデカいのでその差がほぼ無視でき、正規化層との互換性を保つため。\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e1823",
   "metadata": {},
   "source": [
    "- `self.scale` : 入力と同じ次元。訓練時に処理するデータのスケーリングを学習。\n",
    "- `self.shift` : 入力と同じ次元。訓練時に処理するデータのシフトを学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdefb5",
   "metadata": {},
   "source": [
    "層を介して適切な処理を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64cecbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f07e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
